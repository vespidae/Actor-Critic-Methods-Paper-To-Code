{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2bc75d-21c2-4d85-aa7c-f8e7a6badf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02cdaa-b845-497c-9364-a3396e5937f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dim, n_actions, hlOne=256, hlTwo=256):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(*input_dim,hlOne)\n",
    "        self.hidden_layer = nn.Linear(hlOne,hlTwo)\n",
    "        # policy functio\n",
    "        self.pi = nn.Linear(hlTwo, n_actions)\n",
    "        # value function\n",
    "        self.V = nn.Linear(hlTwo, 1)\n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        # device\n",
    "        self.device = [torch.device('cpu')]\n",
    "        if torch.cuda.device_count() > 0:\n",
    "            self.device = []\n",
    "            for d in range(torch.cuda.device_count()):\n",
    "                self.device.append(torch.device('cuda:%s' % d))\n",
    "        self.target_device = 0\n",
    "        # self.to(self.device[0])\n",
    "        self.route_data(self)\n",
    "        ## print('self.device: %s' % self.device)\n",
    "        \n",
    "    def route_data(self, data):\n",
    "        ## print('self.target_device: %s' % self.target_device)\n",
    "        moved_data = data.to(self.device[self.target_device])\n",
    "        # self.target_device = 0 if self.target_device >= len(self.device) else self.target_device + 1\n",
    "        \n",
    "        return moved_data\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.input_layer(state))\n",
    "        x = F.relu(self.hidden_layer(x))\n",
    "        \n",
    "        pi = self.pi(x)\n",
    "        V = self.V(x)\n",
    "        \n",
    "        return (pi, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa5d81-4286-4072-a2f6-f0ab963d36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, lr, gamma, input_dims, n_actions=4, hlOne=256, hlTwo=256):\n",
    "        # set hyperparameters\n",
    "        self.lr, self.gamma = lr, gamma\n",
    "        \n",
    "        # # set memory\n",
    "        # self.action_memory, self.reward_memory = [], []\n",
    "        \n",
    "        self.hlOne, self.hlTwo = hlOne, hlTwo\n",
    "        \n",
    "        self.actor_critic = ActorCriticNetwork(lr, input_dims, n_actions, hlOne, hlTwo)\n",
    "        \n",
    "        self.log_prob = None\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        state = torch.Tensor([observation])\n",
    "        state = self.actor_critic.route_data(state)        \n",
    "        actor_policy, _ = self.actor_critic.forward(state)\n",
    "        \n",
    "        # get action from output\n",
    "        actor_policy = F.softmax(actor_policy, dim=1)\n",
    "        action_probs = torch.distributions.Categorical(actor_policy)\n",
    "        action = action_probs.sample()\n",
    "        \n",
    "        # prepare loss profile\n",
    "        self.log_prob = action_probs.log_prob(action)\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def learn(self, state_null, reward, state_prime, done):\n",
    "        state_null = self.actor_critic.route_data(torch.Tensor([state_null]))\n",
    "        reward = self.actor_critic.route_data(torch.Tensor([reward]))\n",
    "        state_prime = self.actor_critic.route_data(torch.Tensor([state_prime]))\n",
    "        \n",
    "        self.actor_critic.optimizer.zero_grad()\n",
    "        \n",
    "        _, critic_value_null = self.actor_critic.forward(state_null)\n",
    "        _, critic_value_prime = self.actor_critic.forward(state_prime)\n",
    "        \n",
    "        delta = reward + self.gamma*critic_value_prime*(1 - int(done))\n",
    "        \n",
    "        actor_loss = -self.log_prob*delta\n",
    "        critic_loss = delta**2\n",
    "        \n",
    "        (actor_loss + critic_loss).backward()\n",
    "        \n",
    "        self.actor_critic.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9258d1e-23de-44b4-adff-aaea9afe316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(scores, x, figure_file):\n",
    "    # running_avg = np.zeros_like(scores)\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    \n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title(\"Running Average of Previous 100 Scores\")\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d404d-6fa8-421d-8c29-21e59941cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 2000\n",
    "lr = 5e-6\n",
    "gamma = 0.99\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "agent = Agent(lr, gamma, [8], 4, 2048, 1536)\n",
    "\n",
    "figure_name = 'ACTOR_CRITIC-' + 'lunar_lander-%s' % str(agent.hlOne) + 'x%s' % str(agent.hlTwo) + \\\n",
    "    '-lr_%s' % str(agent.lr)  + '-' + str(n_games) + '_games'\n",
    "figure_file = 'plots/' + figure_name + '.png'\n",
    "\n",
    "scores = []\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    obs_null = env.reset()\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(obs_null)\n",
    "        obs_prime, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        agent.learn(obs_null, reward, obs_prime, done)\n",
    "        \n",
    "        obs_null = obs_prime\n",
    "    \n",
    "    scores.append(score)\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print(\"Episode: {}\\tScore: {}\\t\\tAverage Score: {}\".format(i,score,avg_score))\n",
    "    \n",
    "x = [i+1 for i in range(len(scores))]\n",
    "plot_learning_curve(scores, x, figure_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
