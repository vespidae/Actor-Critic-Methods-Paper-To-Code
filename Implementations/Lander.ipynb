{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25815e65-55d7-4ab7-9dc3-22a4a3afbdff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5783,
     "status": "ok",
     "timestamp": 1637282721211,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 300
    },
    "id": "SBo3JYvjpby7",
    "outputId": "eb7e533e-910a-4d35-c9cf-6feab605b4ee"
   },
   "source": [
    "!pip3 install box2d-py\n",
    "!pip3 install gym[Box_2D]\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e524240-c789-447e-8c10-919b3dc66868",
   "metadata": {
    "id": "3e524240-c789-447e-8c10-919b3dc66868"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XvKS59LGo6Dy",
   "metadata": {
    "id": "XvKS59LGo6Dy"
   },
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "n_games = 100\n",
    "\n",
    "for i in range(n_games):\n",
    "    obs_nul = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs_prime, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        # env.render()\n",
    "    print(\"Episode: {}\\t Reward:{}\".format(i,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2dfe3-06e6-4453-bba3-c0e35cdf91dd",
   "metadata": {
    "id": "6ec2dfe3-06e6-4453-bba3-c0e35cdf91dd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a69247-0e83-4484-88a0-5805b1095794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae766996-acf7-41b2-8535-14b7b8e37798",
   "metadata": {
    "id": "ae766996-acf7-41b2-8535-14b7b8e37798"
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(*input_dims,128)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(128,n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        self.device = [torch.device('cpu')]\n",
    "        if torch.cuda.device_count() > 0:\n",
    "            self.device = []\n",
    "            for d in range(torch.cuda.device_count()):\n",
    "                self.device.append(torch.device('cuda:%s' % d))\n",
    "        self.target_device = 0\n",
    "        # self.to(self.device[0])\n",
    "        self.route_data(self)\n",
    "        ## print('self.device: %s' % self.device)\n",
    "        \n",
    "    def route_data(self, data):\n",
    "        ## print('self.target_device: %s' % self.target_device)\n",
    "        moved_data = data.to(self.device[self.target_device])\n",
    "        # self.target_device = 0 if self.target_device >= len(self.device) else self.target_device + 1\n",
    "        \n",
    "        return moved_data\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-4nmIZ5H9UPv",
   "metadata": {
    "id": "-4nmIZ5H9UPv"
   },
   "outputs": [],
   "source": [
    "class PolicyGradientAgent():\n",
    "        def __init__(self, lr, input_dims, gamma=0.99, n_actions=4):\n",
    "            # hyperparameters\n",
    "            self.lr, self.gamma = lr, gamma\n",
    "\n",
    "            # memory\n",
    "            self.reward_memory = []\n",
    "            self.action_memory = []\n",
    "\n",
    "            # policy\n",
    "            self.policy = PolicyNetwork(lr, input_dims, n_actions)\n",
    "        \n",
    "        def choose_action(self, observation):\n",
    "            # convert observation to tensor on device\n",
    "            state = torch.Tensor([observation])\n",
    "            state = self.policy.route_data(state)\n",
    "            \n",
    "            # activation function\n",
    "            probabilities = F.softmax(self.policy.forward(state),1)\n",
    "            \n",
    "            # convert softmax (i.e. calculated probability distribution) to categorical distribution from which we can apply sampling functions\n",
    "            action_probs = torch.distributions.Categorical(probabilities)\n",
    "            \n",
    "            # sample from distribution\n",
    "            action = action_probs.sample()\n",
    "            \n",
    "            # save log probabilities to feed into loss function\n",
    "            log_probs = action_probs.log_prob(action)            \n",
    "            self.action_memory.append(log_probs)\n",
    "            \n",
    "            return action.item()\n",
    "        \n",
    "        def store_rewards(self, reward):\n",
    "            self.reward_memory.append(reward)\n",
    "            \n",
    "        def learn(self):\n",
    "            self.policy.optimizer.zero_grad()\n",
    "            \n",
    "            # calculate time step returns\n",
    "            G = np.zeros_like(self.reward_memory)\n",
    "            for t in range(len(self.reward_memory)):\n",
    "                G_sum = 0\n",
    "                discount = 1\n",
    "                for k in range(t, len(self.reward_memory)):\n",
    "                    G_sum += self.reward_memory[k] * discount\n",
    "                    discount *= self.gamma\n",
    "                G[t] = G_sum\n",
    "            G_raw = torch.Tensor(G)\n",
    "            G = self.policy.route_data(G_raw)\n",
    "            \n",
    "            loss = 0\n",
    "            for g, logprob in zip(G, self.action_memory):\n",
    "                loss += -g * logprob\n",
    "            loss.backward()\n",
    "            self.policy.optimizer.step()\n",
    "            \n",
    "            self.action_memory = []\n",
    "            self.reward_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204102a-d4d4-4a0b-a990-de7f41b06497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(scores, x, figure_file):\n",
    "    # running_avg = np.zeros_like(scores)\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    \n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title(\"Running Average of Previous 100 Scores\")\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0fd0f-4c11-4152-9103-86d4f56134ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 3000\n",
    "gamma=0.99\n",
    "lr = 0.0005\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "agent = PolicyGradientAgent(gamma=gamma, lr=lr, input_dims=[8], n_actions=4)\n",
    "\n",
    "figure_name = 'REINFORCE-' + 'lunar_lander_lr_%s' % str(agent.lr) + '-' + str(n_games) + '_games'\n",
    "figure_file = 'plots/' + figure_name + '.png'\n",
    "\n",
    "scores = []\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    obs_null = env.reset()\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(obs_null)\n",
    "        obs_prime, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "        agent.store_rewards(reward)\n",
    "        \n",
    "        obs_null = obs_prime\n",
    "    agent.learn()\n",
    "    scores.append(score)\n",
    "    \n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print(\"Episode: {}\\tScore: {}\\t\\tAverage Score: {}\".format(i,score,avg_score))\n",
    "    \n",
    "x = [i+1 for i in range(len(scores))]\n",
    "plot_learning_curve(scores, x, figure_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lander.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/vespidae/Actor-Critic-Methods-Paper-To-Code/blob/master/Implementations/Lander.ipynb",
     "timestamp": 1637288647938
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
