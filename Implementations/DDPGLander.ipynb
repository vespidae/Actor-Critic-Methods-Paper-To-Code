{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6710142a-9420-4817-9843-bf8604b2c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82643e0-a329-41c6-ab44-fbed7a37e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise():\n",
    "    # initialize mean, std, \n",
    "    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, xNull=None):\n",
    "        self.mu, self.sigma, self.theta, self.dt, self.xNull = mu, sigma, theta, dt, xNull\n",
    "        self.reset()\n",
    "        \n",
    "    # allows us to use the name of an object as a function\n",
    "    def __call__(self):\n",
    "        # get temporal correlation of noise\n",
    "        x = self.xPrevious + self.theta * (self.mu - self.xPrevious) * \\\n",
    "        self.dt + self.sigma * np.sqrt(self.dt) + np.random.normal(size=self.mu.shape)\n",
    "        self.xPrevious = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # set initial value of xPrevious\n",
    "    def reset(self):\n",
    "        self.xPrevious = self.xNull if self.xNull is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f680dc11-e273-4e06-a5af-3e9aa78df562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3f31bb-74a1-4be4-8595-f1fc102b0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape, action_shape):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        self.memory = {\n",
    "            \"null_state\" : np.zeros((self.mem_size, *input_shape)),\n",
    "            \"prime_state\" : np.zeros((self.mem_size, *input_shape)),\n",
    "            \"action\" : np.zeros((self.mem_size, action_shape)),\n",
    "            \"reward\" : np.zeros(self.mem_size),\n",
    "            \"terminal\" : np.zeros(self.mem_size, dtype=np.bool),\n",
    "        }\n",
    "        \n",
    "        # mask for setting critic values for new state to zero\n",
    "        # self.term_mem = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "        \n",
    "    def store_transition(self, null_state, action, reward, prime_state, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        alignment = zip([\"null_state\", \"prime_state\", \"action\", \"reward\"],\\\n",
    "                        [null_state, action, reward, prime_state])\n",
    "        \n",
    "        for mem, value in alignment:\n",
    "            self.memory[mem][self.mem_trgt_indx] = value\n",
    "            \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "#     def sample_replay(self, proportion):\n",
    "        \n",
    "#         sample_size = np.ceil(len(self.replays) * proportion)\n",
    "#         return np.random.choice(self.replays, sample_size)\n",
    "        \n",
    "    def sample_replay(self, batch_size):\n",
    "        picks = {}\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)        \n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "        \n",
    "        for mem in [\"null_state\", \"prime_state\", \"action\", \"reward\", \"terminal\"]:\n",
    "            picks[mem] = self.memory[mem][batch]\n",
    "            \n",
    "        return picks[\"null_state\"], picks[\"prime_state\"], picks[\"action\"], picks[\"reward\"], picks[\"terminal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08bb47be-1eee-4749-b41a-dd25b5640bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac80a4d1-8c74-45b0-8323-5928d6959ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, hl_one, hl_two, n_actions, chkpt_name, chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self.beta, self.input_dims, self.hl_one, self.hl_two, self.n_actions, self.chkpt_name, self.chkpt_dir = \\\n",
    "        beta, input_dims, hl_one, hl_two, n_actions, chkpt_name, chkpt_dir\n",
    "        \n",
    "        chkpt_file = os.path.join(self.chkpt_dir + self.chkpt_name + '_ddpg')\n",
    "        \n",
    "        # define layers\n",
    "        self.input = nn.Linear(*self.input_dims, self.hl_one)\n",
    "        self.hidden = nn.Linear(self.hl_one, self.hl_two)\n",
    "        \n",
    "        # define normalizers\n",
    "        self.lnormi = nn.LayerNorm(self.hl_one)\n",
    "        self.lnormh = nn.LayerNorm(self.hl_two)\n",
    "        \n",
    "        # a calculation\n",
    "        self.action_output = nn.Linear(self.n_actions, self.hl_two)\n",
    "        \n",
    "        # Q calculation\n",
    "        self.critic_output = nn.Linear(self.hl_two, 1)\n",
    "        \n",
    "        # initialize layers\n",
    "        for layer in [self.input, self.hidden, self.action_output]:\n",
    "            fan_in = 1 / np.sqrt(layer.weight.data.size()[0])\n",
    "            layer.weight.data.uniform_(-fan_in, fan_in)\n",
    "            layer.bias.data.uniform_(-fan_in, fan_in)  \n",
    "            \n",
    "        critic_fan_in = 0.003\n",
    "        self.critic_output.weight.data.uniform_(-critic_fan_in, critic_fan_in)\n",
    "        self.critic_output.bias.data.uniform_(-critic_fan_in, critic_fan_in)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.beta, weight_decay=0.01)\n",
    "        \n",
    "        # device\n",
    "        self.device = [torch.device('cpu')]\n",
    "        if torch.cuda.device_count() > 0:\n",
    "            self.device = []\n",
    "            for d in range(torch.cuda.device_count()):\n",
    "                self.device.append(torch.device('cuda:%s' % d))\n",
    "        self.target_device = 0\n",
    "        # self.to(self.device[0])\n",
    "        self.route_data(self)\n",
    "        ## print('self.device: %s' % self.device)\n",
    "        \n",
    "    def route_data(self, data):\n",
    "        ## print('self.target_device: %s' % self.target_device)\n",
    "        moved_data = data.to(self.device[self.target_device])\n",
    "        # self.target_device = 0 if self.target_device >= len(self.device) else self.target_device + 1\n",
    "        \n",
    "        return moved_data\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        state_value = self.lnormi(self.input(state))\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.lnormh(self.hidden(state_value))\n",
    "        action_value = self.action_output(action)\n",
    "        state_action_value = F.relu(torch.add(state_value, action_value))\n",
    "        state_action_value = self.critic_output(state_action_value)\n",
    "        \n",
    "        return state_action_value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        print(\"Saving checkpoint...\")\n",
    "        torch.save(self.state_dict(), self.chkpt_file)\n",
    "        print(\"Checkpoint saved.\")\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        self.load_state_dict(torch.load(self.chkpt_file))\n",
    "        print(\"Checkpoint loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4c6f50-8b7d-47d2-96b9-1cc8bbeeea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, hl_one, hl_two, n_actions, chkpt_name, chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.alpha, self.input_dims, self.hl_one, self.hl_two, self.n_actions, self.chkpt_name, self.chkpt_dir = \\\n",
    "        alpha, input_dims, hl_one, hl_two, n_actions, chkpt_name, chkpt_dir\n",
    "        \n",
    "        self.chkpt_file = os.path.join(self.chkpt_dir + self.chkpt_name + '_ddpg')\n",
    "        \n",
    "        # define layers\n",
    "        self.input = nn.Linear(*self.input_dims, self.hl_one)\n",
    "        self.hidden = nn.Linear(self.hl_one, self.hl_two)\n",
    "        # self.output = nn.Linear(self.hl_two, self.n_actions)\n",
    "        \n",
    "        # define normalizers\n",
    "        self.lnormi = nn.LayerNorm(self.hl_one)\n",
    "        self.lnormh = nn.LayerNorm(self.hl_two)\n",
    "        \n",
    "        # define mu\n",
    "        self.mu = nn.Linear(self.hl_two, self.n_actions)\n",
    "        \n",
    "        # initialize layers\n",
    "        for layer in [self.input, self.hidden]:\n",
    "            fan_in = 1 / np.sqrt(layer.weight.data.size()[0])\n",
    "            layer.weight.data.uniform_(-fan_in,fan_in)\n",
    "            layer.bias.data.uniform_(-fan_in,fan_in)\n",
    "            \n",
    "        mu_fan_in = 3e-3\n",
    "        self.mu.weight.data.uniform_(-mu_fan_in, mu_fan_in)\n",
    "        self.mu.bias.data.uniform_(-mu_fan_in, mu_fan_in)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.alpha)#, weight_decay=1e-4)\n",
    "        \n",
    "        # device\n",
    "        self.device = [torch.device('cpu')]\n",
    "        if torch.cuda.device_count() > 0:\n",
    "            self.device = []\n",
    "            for d in range(torch.cuda.device_count()):\n",
    "                self.device.append(torch.device('cuda:%s' % d))\n",
    "        self.target_device = 0\n",
    "        # self.to(self.device[0])\n",
    "        self.route_data(self)\n",
    "        ## print('self.device: %s' % self.device)\n",
    "        \n",
    "    def route_data(self, data):\n",
    "        ## print('self.target_device: %s' % self.target_device)\n",
    "        moved_data = data.to(self.device[self.target_device])\n",
    "        # self.target_device = 0 if self.target_device >= len(self.device) else self.target_device + 1\n",
    "        \n",
    "        return moved_data\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.lnormi(self.input(state)))\n",
    "        x = F.relu(self.lnormh(self.hidden(x)))\n",
    "        A = F.tanh(self.mu(x))\n",
    "        # action_value = self.mu(x)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        print(\"Saving checkpoint...\")\n",
    "        torch.save(self.state_dict(), self.chkpt_file)\n",
    "        print(\"Checkpoint saved.\")\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        print(\"Loading checkpoint...\")\n",
    "        self.load_state_dict(torch.load(self.chkpt_file))\n",
    "        print(\"Checkpoint loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e49175d-18d8-41e9-8b87-baf2438d5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, alpha, beta, tau, input_dims, n_actions, gamma=0.99, hlOne=400, hlTwo=300, buffer_size=1e6, batch_size=64):\n",
    "        self.alpha, self.beta, self.gamma, self.tau, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, self.buffer_size, self.batch_size = \\\n",
    "        alpha, beta, gamma, tau, input_dims, n_actions, hlOne, hlTwo, buffer_size, batch_size\n",
    "        \n",
    "        self.actor = ActorNetwork(self.alpha, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, 'actor')\n",
    "        self.critic = CriticNetwork(self.beta, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, 'critic')\n",
    "        self.actor_prime = ActorNetwork(self.alpha, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, 'target_actor')\n",
    "        self.critic_prime = CriticNetwork(self.beta, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, 'target_critic')\n",
    "        \n",
    "        self.buffer = ReplayBuffer(self.buffer_size, self.input_dims, self.n_actions)\n",
    "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        # get policy\n",
    "        state = torch.Tensor([observation])\n",
    "        state = self.actor.route_data(state)\n",
    "        mu = self.actor(state)\n",
    "        mu_prime = mu + torch.Tensor(self.noise)\n",
    "        self.actor.train()\n",
    "        \n",
    "#         # get action from policy\n",
    "#         mu = F.softmax(mu, dim=1)\n",
    "#         action_probs = torch.distributions.Categorical(mu)\n",
    "#         a = action_probs.sample()\n",
    "        \n",
    "#         self.log_prob = action_probs.log_prob(a)\n",
    "        \n",
    "        return mu_prime.cpu().detach(),numpy()[0]\n",
    "        \n",
    "    def remember(self, state_null, action, reward, state_prime, done):\n",
    "        self.buffer.store_transition(state_null, action, reward, state_prime, done)\n",
    "    \n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.actor_prime.save_checkpoint()\n",
    "        self.critic_prime.save_checkpoint()\n",
    "    \n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.actor_prime.load_checkpoint()\n",
    "        self.critic_prime.load_checkpoint()\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.buffer.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        null_states, actions, rewards, prime_states, terminal = self.buffer.sample_replay(self.batch_size)\n",
    "        null_states = self.actor.route_data(torch.Tensor([null_states]))\n",
    "        actions = self.actor.route_data(torch.Tensor([actions]))\n",
    "        rewards = self.actor.route_data(torch.Tensor([rewards]))\n",
    "        prime_states = self.actor.route_data(torch.Tensor([prime_states]))\n",
    "        terminal = self.actor.route_data(torch.Tensor([terminal]))\n",
    "\n",
    "        # target_critic_value_null = self.critic_prime(state_null)\n",
    "        # target_critic_value_prime = self.critic_prime(state_prime)\n",
    "\n",
    "        Q = self.critic(null_state, actions)\n",
    "        Q_prime = self.critic_prime(prime_state, self.actor_prime(prime_state))\n",
    "        # use terminal tensor as a mask to modify respective rewards\n",
    "        Q_prime[terminal] = 0.0\n",
    "        Q_prime = Q_prime.view[-1]\n",
    "        \n",
    "        y = reward + self.gamma * Q_prime\n",
    "        y = y.view(self.batch_size, 1)\n",
    "\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss = -self.critic(null_states, self.actor(null_states))\n",
    "        actor_loss = torch.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "        \n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = F.mse_loss(y, Q)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "        \n",
    "        self.update_network_parameters()\n",
    "        \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        theta_mu = self.actor.state_dict()\n",
    "        theta_Q = self.critic.state_dict()\n",
    "        theta_mu_prime = self.actor_prime.state_dict()\n",
    "        theta_Q_prime = self.critic_prime.state_dict()\n",
    "        # theta_mu = {name:param for name,param in self.actor.named_parameters()}\n",
    "        # theta_Q = {name:param for name,param in self.critic.named_parameters()}\n",
    "        # theta_mu_prime = {name:param for name,param in self.actor_prime.named_parameters()}\n",
    "        # theta_Q_prime = {name:param for name,param in self.critic_prime.named_parameters()}\n",
    "        \n",
    "        for target_network, null_network in [[theta_mu_prime,theta_mu],[theta_Q_prime,theta_Q]]:\n",
    "            for param in null_network.keys():\n",
    "                target_network[param] = (tau * null_network[param].clone()) + \\\n",
    "                ((1 - tau) * target_network[param].clone())\n",
    "        \n",
    "        self.actor_prime.load_state_dict(theta_mu_prime)\n",
    "        self.critic_prime.load_state_dict(theta_Q_prime)\n",
    "        \n",
    "#         L = (1 / batch_size) * ((y - Q) ** 2)\n",
    "#         # target_actor_loss = -self.log_prob*delta\n",
    "#         # target_critic_loss = delta**2\n",
    "\n",
    "#         # (target_actor_loss + target_critic_loss).backward()\n",
    "#         L.backward()\n",
    "\n",
    "#         self.actor.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85003b-580f-40c9-87cd-f717b6de0bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
