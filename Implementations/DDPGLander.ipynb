{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710142a-9420-4817-9843-bf8604b2c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82643e0-a329-41c6-ab44-fbed7a37e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise():\n",
    "    # initialize mean, std, \n",
    "    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, xNull=None):\n",
    "        self.mu, self.sigma, self.theta, self.dt, self.xNull = mu, sigma, theta, dt, xNull\n",
    "        self.reset()\n",
    "        \n",
    "    # allows us to use the name of an object as a function\n",
    "    def __call__(self):\n",
    "        # get temporal correlation of noise\n",
    "        x = self.xPrevious + self.theta * (self.mu - self.xPrevious) * \\\n",
    "        self.dt + self.sigma * np.sqrt(self.dt) + np.random.normal(size=self.mu.shape)\n",
    "        self.xPrevious = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # set initial value of xPrevious\n",
    "    def reset(self):\n",
    "        self.xPrevious = self.xNull if self.xNull is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f31bb-74a1-4be4-8595-f1fc102b0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape, action_shape):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        self.memory = {\n",
    "            \"null_state\" : np.array([np.zeros(input_shape) for reg in range(self.mem_size)]),# np.zeros((self.mem_size, *input_shape)),\n",
    "            \"action\" : np.zeros((self.mem_size, action_shape)),\n",
    "            \"reward\" : np.zeros(self.mem_size),\n",
    "            \"prime_state\" : np.array([np.zeros(input_shape) for reg in range(self.mem_size)]),#np.zeros((self.mem_size, *input_shape)),\n",
    "            \"terminal\" : np.zeros(self.mem_size, dtype=bool),\n",
    "        }\n",
    "        \n",
    "        # mask for setting critic values for new state to zero\n",
    "        # self.term_mem = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "        \n",
    "    def store_transition(self, null_state, action, reward, prime_state, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        alignment = zip([\"null_state\", \"action\", \"reward\", \"prime_state\"],                        [null_state, action, reward, prime_state])\n",
    "        \n",
    "        for mem, value in alignment:\n",
    "            # print(\"mem: {}\\tself.mem_cntr: {}\\tvalue.shape: {}\\tself.memory[mem][self.mem_cntr].shape: {}\".format(\\\n",
    "            #     mem, self.mem_cntr, value.shape, self.memory[mem][self.mem_cntr].shape))\n",
    "            # if mem not \"null_state\" and not \"prime_state\":\n",
    "            #     print(\"Attr: {}, Index: {}, Value: {}\".format(mem, self.mem_cntr % self.mem_size, value))\n",
    "            self.memory[mem][index] = value\n",
    "            \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "#     def sample_replay(self, proportion):\n",
    "        \n",
    "#         sample_size = np.ceil(len(self.replays) * proportion)\n",
    "#         return np.random.choice(self.replays, sample_size)\n",
    "        \n",
    "    def sample_replay(self, batch_size):\n",
    "        picks = {}\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)        \n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "        \n",
    "        for mem in [\"null_state\", \"prime_state\", \"action\", \"reward\", \"terminal\"]:\n",
    "            picks[mem] = self.memory[mem][batch]\n",
    "            \n",
    "        return picks[\"null_state\"], picks[\"action\"], picks[\"reward\"], picks[\"prime_state\"], picks[\"terminal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb47be-1eee-4749-b41a-dd25b5640bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac80a4d1-8c74-45b0-8323-5928d6959ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, n_actions, hl_one, hl_two, chkpt_name, chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self.beta, self.input_dims, self.hl_one, self.hl_two, self.n_actions, self.chkpt_name, self.chkpt_dir =         beta, input_dims, hl_one, hl_two, n_actions, chkpt_name, chkpt_dir\n",
    "        \n",
    "        self.chkpt_file = os.path.join(self.chkpt_dir, self.chkpt_name + '_ddpg')\n",
    "        \n",
    "        # define layers\n",
    "        # self.frame = nn.Conv2d(3,1,3)\n",
    "        self.flatten = nn.Flatten().to('cuda:0')\n",
    "        self.input = nn.Linear(np.prod(self.input_dims),self.hl_one).to('cuda:0')\n",
    "        self.hidden = nn.Linear(self.hl_one, self.hl_two).to('cuda:1')\n",
    "        \n",
    "        # define normalizers\n",
    "        self.lnormi = nn.LayerNorm(self.hl_one).to('cuda:0')\n",
    "        self.lnormh = nn.LayerNorm(self.hl_two).to('cuda:1')\n",
    "        \n",
    "        # a calculation\n",
    "        self.action_output = nn.Linear(self.n_actions, self.hl_two).to('cuda:1')\n",
    "        \n",
    "        # Q calculation\n",
    "        self.critic_output = nn.Linear(self.hl_two, 1).to('cuda:1')\n",
    "        \n",
    "        # initialize layers\n",
    "        for layer in [self.input, self.hidden, self.action_output]:\n",
    "            fan_in = 1 / np.sqrt(layer.weight.data.size()[0])\n",
    "            layer.weight.data.uniform_(-fan_in, fan_in)\n",
    "            layer.bias.data.uniform_(-fan_in, fan_in)  \n",
    "            \n",
    "        critic_fan_in = 0.003\n",
    "        self.critic_output.weight.data.uniform_(-critic_fan_in, critic_fan_in)\n",
    "        self.critic_output.bias.data.uniform_(-critic_fan_in, critic_fan_in)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.beta, weight_decay=0.01)\n",
    "        \n",
    "        # device\n",
    "        self.device = [torch.device('cpu')]\n",
    "        if torch.cuda.device_count() > 0:\n",
    "            self.device = []\n",
    "            for d in range(torch.cuda.device_count()):\n",
    "                self.device.append(torch.device('cuda:%s' % d))\n",
    "        self.target_device = 1\n",
    "        # self.to(self.device[0])\n",
    "        ##self.route_data(self)\n",
    "        ## print('self.device: %s' % self.device)\n",
    "        \n",
    "    def route_data(self, data):\n",
    "        ## print('self.target_device: %s' % self.target_device)\n",
    "        moved_data = data.to(self.device[self.target_device])\n",
    "        # self.target_device = 0 if self.target_device >= len(self.device) else self.target_device + 1\n",
    "        \n",
    "        return moved_data\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        state_value = self.flatten(state.to('cuda:0'))\n",
    "        state_value = self.lnormi(self.input(state_value))\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.lnormh(self.hidden(state_value.to('cuda:1')))\n",
    "        \n",
    "        action_value = self.action_output(action.to('cuda:1'))\n",
    "        \n",
    "        state_action_value = F.relu(torch.add(state_value, action_value))\n",
    "        state_action_value = self.critic_output(state_action_value)\n",
    "        \n",
    "        return state_action_value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        print(\"saving {} checkpoint...\".format(self.chkpt_name))\n",
    "        torch.save(self.state_dict(), self.chkpt_file)\n",
    "        print(\"{} checkpoint saved.\".format(self.chkpt_name))\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        print(\"loading {} checkpoint...\".format(self.chkpt_name))\n",
    "        self.load_state_dict(torch.load(self.chkpt_file))\n",
    "        print(\"{} checkpoint loaded.\".format(self.chkpt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279f230-3108-49c0-844a-5ca84559344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bc629-3e94-41a8-8b0d-93462deb3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, n_actions, hl_one, hl_two, chkpt_name, chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.alpha, self.input_dims, self.hl_one, self.hl_two, self.n_actions, self.chkpt_name, self.chkpt_dir =         alpha, input_dims, hl_one, hl_two, n_actions, chkpt_name, chkpt_dir\n",
    "        \n",
    "        self.chkpt_file = os.path.join(self.chkpt_dir, self.chkpt_name + '_ddpg')\n",
    "        \n",
    "        # define layers\n",
    "        # self.frame = nn.Conv2d(3,1,3)\n",
    "        self.flatten = nn.Flatten().to('cuda:0')\n",
    "        self.input = nn.Linear(np.prod(self.input_dims),self.hl_one).to('cuda:0')\n",
    "        self.hidden = nn.Linear(self.hl_one, self.hl_two).to('cuda:1')\n",
    "        # self.output = nn.Linear(self.hl_two, self.n_actions)\n",
    "        \n",
    "        # define normalizers\n",
    "        self.lnormi = nn.LayerNorm(self.hl_one).to('cuda:0')\n",
    "        self.lnormh = nn.LayerNorm(self.hl_two).to('cuda:1')\n",
    "        \n",
    "        # define mu\n",
    "        self.mu = nn.Linear(self.hl_two, self.n_actions).to('cuda:1')\n",
    "        \n",
    "        # initialize layers\n",
    "        for layer in [self.input, self.hidden]:\n",
    "            fan_in = 1 / np.sqrt(layer.weight.data.size()[0])\n",
    "            layer.weight.data.uniform_(-fan_in,fan_in)\n",
    "            layer.bias.data.uniform_(-fan_in,fan_in)\n",
    "            \n",
    "        mu_fan_in = 3e-3\n",
    "        self.mu.weight.data.uniform_(-mu_fan_in, mu_fan_in)\n",
    "        self.mu.bias.data.uniform_(-mu_fan_in, mu_fan_in)\n",
    "        \n",
    "        # define optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.alpha)#, weight_decay=1e-4)\n",
    "        \n",
    "        # device\n",
    "        self.device = [torch.device('cpu')]\n",
    "        if torch.cuda.device_count() > 0:\n",
    "            self.device = []\n",
    "            for d in range(torch.cuda.device_count()):\n",
    "                self.device.append(torch.device('cuda:%s' % d))\n",
    "        self.target_device = 1\n",
    "        # self.to(self.device[0])\n",
    "        ##self.route_data(self)\n",
    "        ## print('self.device: %s' % self.device)\n",
    "        \n",
    "    def route_data(self, data):\n",
    "        ## print('self.target_device: %s' % self.target_device)\n",
    "        moved_data = data.to(self.device[self.target_device])\n",
    "        # self.target_device = 0 if self.target_device >= len(self.device) else self.target_device + 1\n",
    "        \n",
    "        return moved_data\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # print(state.shape)\n",
    "        # print(self.frame)\n",
    "        # print(self.flatten(state).shape)\n",
    "        # plt.imshow(state[0].cpu())\n",
    "        x = self.flatten(state.to('cuda:0'))\n",
    "        x = F.relu(self.lnormi(self.input(x)))\n",
    "        x = F.relu(self.lnormh(self.hidden(x.to('cuda:1'))))\n",
    "        A = torch.tanh(self.mu(x))\n",
    "        # action_value = self.mu(x)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        print(\"saving {} checkpoint...\".format(self.chkpt_name))\n",
    "        torch.save(self.state_dict(), self.chkpt_file)\n",
    "        print(\"{} checkpoint saved.\".format(self.chkpt_name))\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        print(\"loading {} checkpoint...\".format(self.chkpt_name))\n",
    "        self.load_state_dict(torch.load(self.chkpt_file))\n",
    "        print(\"{} checkpoint loaded.\".format(self.chkpt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49175d-18d8-41e9-8b87-baf2438d5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, alpha, beta, tau, input_dims, n_actions, gamma=0.99, hlOne=400, hlTwo=300, buffer_size=1e6, batch_size=64):\n",
    "        self.alpha, self.beta, self.gamma, self.tau, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, self.buffer_size, self.batch_size =         alpha, beta, gamma, tau, input_dims, n_actions, hlOne, hlTwo, buffer_size, batch_size\n",
    "        # for argument,name in zip([self.alpha, self.beta, self.gamma, self.tau, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, self.buffer_size, self.batch_size],[\"alpha\", \"beta\", \"gamma\", \"tau\", \"input_dims\", \"n_actions\", \"hlOne\", \"hlTwo\", \"buffer_size\", \"batch_size\"]):\n",
    "        #     print(\"Argument: {}\\tName: {}\".format(name, argument))\n",
    "        \n",
    "        self.actor = ActorNetwork(self.alpha, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, 'actor')\n",
    "        self.critic = CriticNetwork(self.beta, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, 'critic')\n",
    "        self.actor_prime = ActorNetwork(self.alpha, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, 'target_actor')\n",
    "        self.critic_prime = CriticNetwork(self.beta, self.input_dims, self.n_actions, self.hlOne, self.hlTwo, 'target_critic')\n",
    "        \n",
    "        self.buffer = ReplayBuffer(self.buffer_size, self.input_dims, self.n_actions)\n",
    "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        # get policy\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float)\n",
    "        ##state = self.actor.route_data(state)\n",
    "        mu = self.actor(state)#.to('cuda:0')\n",
    "        mu_prime = mu + self.actor.route_data(torch.Tensor(self.noise()))\n",
    "        self.actor.train()\n",
    "        \n",
    "#         # get action from policy\n",
    "#         mu = F.softmax(mu, dim=1)\n",
    "#         action_probs = torch.distributions.Categorical(mu)\n",
    "#         a = action_probs.sample()\n",
    "        \n",
    "#         self.log_prob = action_probs.log_prob(a)\n",
    "        \n",
    "        return mu_prime.cpu().detach().numpy()[0]\n",
    "        \n",
    "    def remember(self, state_null, action, reward, state_prime, done):\n",
    "        self.buffer.store_transition(state_null, action, reward, state_prime, done)\n",
    "    \n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.actor_prime.save_checkpoint()\n",
    "        self.critic_prime.save_checkpoint()\n",
    "    \n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.actor_prime.load_checkpoint()\n",
    "        self.critic_prime.load_checkpoint()\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.buffer.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        null_states, actions, rewards, prime_states, terminal = self.buffer.sample_replay(self.batch_size)\n",
    "        null_states = self.actor.route_data(torch.tensor(np.array([null_states]), dtype=torch.float))\n",
    "        actions = self.actor.route_data(torch.tensor(np.array([actions]), dtype=torch.float))\n",
    "        rewards = self.actor.route_data(torch.tensor(np.array([rewards]), dtype=torch.float))\n",
    "        prime_states = self.actor.route_data(torch.tensor(np.array([prime_states]), dtype=torch.float))\n",
    "        # print(terminal)\n",
    "        terminal = self.actor.route_data(torch.tensor(np.array([terminal])))\n",
    "\n",
    "        # target_critic_value_null = self.critic_prime(state_null)\n",
    "        # target_critic_value_prime = self.critic_prime(state_prime)\n",
    "\n",
    "        for n, a, r, p, t in zip(null_states, actions, rewards, prime_states, terminal):\n",
    "            Q = self.critic(n, a)\n",
    "            Q_prime = self.critic_prime(p, self.actor_prime(p))\n",
    "            # use terminal tensor as a mask to modify respective rewards\n",
    "            Q_prime[t] = 0.0\n",
    "            Q_prime = Q_prime.view(-1)\n",
    "\n",
    "            y = r + self.gamma * Q_prime\n",
    "            y = y.view(self.batch_size, 1)\n",
    "\n",
    "            self.actor.optimizer.zero_grad()\n",
    "            actor_loss = -self.critic(n, self.actor(n))\n",
    "            actor_loss = torch.mean(actor_loss)\n",
    "            actor_loss.backward()\n",
    "            self.actor.optimizer.step()\n",
    "\n",
    "            self.critic.optimizer.zero_grad()\n",
    "            critic_loss = F.mse_loss(y, Q)\n",
    "            critic_loss.backward()\n",
    "            self.critic.optimizer.step()\n",
    "        \n",
    "        self.update_network_parameters()\n",
    "        \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "            \n",
    "        theta_mu = self.actor.state_dict()\n",
    "        theta_Q = self.critic.state_dict()\n",
    "        theta_mu_prime = self.actor_prime.state_dict()\n",
    "        theta_Q_prime = self.critic_prime.state_dict()\n",
    "        # theta_mu = {name:param for name,param in self.actor.named_parameters()}\n",
    "        # theta_Q = {name:param for name,param in self.critic.named_parameters()}\n",
    "        # theta_mu_prime = {name:param for name,param in self.actor_prime.named_parameters()}\n",
    "        # theta_Q_prime = {name:param for name,param in self.critic_prime.named_parameters()}\n",
    "        \n",
    "        for target_network, null_network in [[theta_mu_prime,theta_mu],[theta_Q_prime,theta_Q]]:\n",
    "            for param in null_network.keys():\n",
    "                target_network[param] = (tau * null_network[param].clone()) +                 ((1 - tau) * target_network[param].clone())\n",
    "        \n",
    "        self.actor_prime.load_state_dict(theta_mu_prime)\n",
    "        self.critic_prime.load_state_dict(theta_Q_prime)\n",
    "        \n",
    "#         L = (1 / batch_size) * ((y - Q) ** 2)\n",
    "#         # target_actor_loss = -self.log_prob*delta\n",
    "#         # target_critic_loss = delta**2\n",
    "\n",
    "#         # (target_actor_loss + target_critic_loss).backward()\n",
    "#         L.backward()\n",
    "\n",
    "#         self.actor.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc837e4-37f8-4f9e-b970-ea40f8b2d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(scores, x, figure_file):\n",
    "    # running_avg = np.zeros_like(scores)\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    \n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title(\"Running Average of Previous 100 Scores\")\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf31eda-d407-4d7d-b0da-cc00313e79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa7b48-30ec-4d9b-a3eb-6bf5d0ad9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 5000\n",
    "actor_lr =1e-4\n",
    "critic_lr = 1e-3\n",
    "soft_target_update = 1e-3\n",
    "# input_dimensions = [210, 160, 3]\n",
    "# number_of_actions = 4\n",
    "discount_factor = 0.99\n",
    "hlOne = 400\n",
    "hlTwo = 300\n",
    "buffer_size = 2e4\n",
    "minibatch_size = 64\n",
    "\n",
    "# instantiate environment and agent\n",
    "# env = gym.make('LunarLanderContinuous-v2') # <- good vizualizations\n",
    "# env = gym.make('ALE/ElevatorAction-v5')#, render_mode='human')\n",
    "# env = gym.make('ALE/Adventure-v5')\n",
    "# env = gym.make('ALE/Asteroids-v5')\n",
    "# env = gym.make('ChopperCommand-v4')\n",
    "# env = gym.make('ALE/ChopperCommand-v5') # <- good vizualizations\n",
    "# env = gym.make('ALE/Alien-v5') # <- good vizualizations\n",
    "env = gym.make('ALE/BattleZone-v5') # <- good vizualizations; buffer size: 2e4\n",
    "\n",
    "# env = gym.make('SuperMarioBros-v0') # <- good vizualizations; buffer size: 7e3\n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "# env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "\n",
    "# print(env.observation_space.shape[0])\n",
    "action_space_is_box = type(env.action_space) is gym.spaces.box.Box\n",
    "number_of_actions = env.action_space.shape[0] if action_space_is_box else env.action_space.n\n",
    "# print(type(env.action_space))\n",
    "agent = Agent(actor_lr, critic_lr, soft_target_update, env.observation_space.shape,               number_of_actions, discount_factor,               hlOne, hlTwo, int(buffer_size), minibatch_size)\n",
    "            # alpha, beta, tau, input_dims, n_actions, gamma=0.99, hlOne=400, hlTwo=300, buffer_size=1e6, batch_size=64\n",
    "# print(agent.actor.n_actions)\n",
    "\n",
    "figure_name = 'ACTOR_CRITIC-' + 'lunar_lander-%s' % str(agent.hlOne) + 'x%s' % str(agent.hlTwo) +     '-alpha_%s' % str(agent.alpha)  + '-beta_%s' % str(agent.beta)  +     '-tau_%s' % str(agent.tau) + '-buffer_%s' % str(agent.buffer_size)  +     '-batch_size_%s' % str(agent.batch_size)  + '-' +     str(n_games) + '_games'\n",
    "figure_file = 'plots/' + figure_name + '.png'\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "\n",
    "for i in range(n_games):\n",
    "    # initialize values\n",
    "    null_obv = env.reset()\n",
    "    agent.noise.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # get action\n",
    "        choice = agent.choose_action(null_obv)\n",
    "        # prime_obv, reward, done, info = env.step(action.argmax(0))\n",
    "        action = choice if action_space_is_box else choice.argmax(0)\n",
    "        prime_obv, reward, done, info = env.step(action)\n",
    "        # env.render()\n",
    "        agent.remember(null_obv, action, reward, prime_obv, done)\n",
    "        # update score\n",
    "        score += reward\n",
    "        # learn\n",
    "        agent.learn()\n",
    "        \n",
    "        # prep for next (time)step\n",
    "        # null_obv = prime_obv if info['lives'] > 0 else env.reset()\n",
    "        # if info['lives'] == 0: done = True\n",
    "        null_obv = prime_obv\n",
    "        \n",
    "    env.close()\n",
    "        \n",
    "    # manage score\n",
    "    score_history.append(score)\n",
    "    running_avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    if running_avg_score > best_score:\n",
    "        best_score = running_avg_score\n",
    "        agent.save_models()\n",
    "        \n",
    "    print(\"Episode: {}\\t\\tScore: {}\\t\\tAverage Score: {}\".format(i, score, running_avg_score))\n",
    "    \n",
    "x = [i+1 for i in range(len(score_history))]\n",
    "plot_learning_curve(score_history, x, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b8f21-fc75-45b8-bf7a-893b6dacc446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
